\label{sec:background}
In this section, we discuss the GPU execution model, focusing on characteristics that lead to the vulnerabilities we propose to study.
%
We also discuss related work on formal models of CUDA and security for GPU programs.
% Extras: 

% One prominent task is inference of neural networks, which process vast amounts of personal data, such as audio, text or images.


% Typically, AI is used for
% the evaluation of large datasets, from processing medical
% data to automated evaluation of surveilance feeds. Today,
% AI suitable for everyday use has arrived at the hands
% of the end-user in the form of Large Language Models
% (LLMs) [

% Hence, a lot of business cases for the utilization of
% LLMs have emerged, ranging from automatically handling
% customer service requests to tasks such as text content
% generation

% offer GPU-asa-
% Service (GaaS), where users can rent GPUs on demand
% and pay via a usage model. Customers can enjoy a high
% degree of flexibility, renting GPU computational power
% on demand.

% Allocating a complete GPU
% to a single container or virtual machine (VM) can be
% considered a waste of resources if either only a fraction
% or only a limited time of processing power is needed. The
% ability to share GPUs gives the provider a cost-effective
% measure to adapt to varying workloads. As industrial
% examples, Googleâ€™s Kubernetis Engine allows to share
% a GPU between up to 48 tenants, while Microsoft build
% GPU Paravirtualization into the Hyper-V Hypervisor, allowing
% VMs to share a single GPU



\paragraph{Parallelism and Memory Behavior in GPU.}
Threads on a GPU (of which there may be thousands) are organized into groups
called {\em warps}.
%
We will use the identifier \texttt{warpsize} to refer to the number of threads
per warp, which is~32 on all implementations of which we're aware.
%
Warps are organized into {\em blocks}, and blocks are grouped into {\em grids}. The dimensions of blocks and grids are specified when the kernel is launched.
%
Each thread has a unique thread ID within its block, and each block has a unique block ID within the grid, enabling scalable parallel execution.
%
GPUs utilize Single Instruction, Multiple Thread (SIMT) execution to run the same arithmetic or logical instruction on many threads simultaneously.
%
All threads within a warp must execute the same instruction, although some threads may remain inactive during execution.

Registers are the fastest type of memory in CUDA and are private to each thread, allowing quick access to frequently used variables. 
%
{\em Shared memory} is a smaller, but faster, memory space accessible to all threads within a block, enabling efficient data sharing and coordination. 
%
{\em Global memory} is the largest memory space, accessible by all threads across different blocks, but it is much slower compared to registers and shared memory.

\paragraph{Shared Memory and Bank Conflicts.}
Shared memory, like other types of memory, must be loaded into registers before operations can be performed on it.
%
Because all active threads within a warp perform the same instruction simultaneously, they will all perform simultaneous requests to shared memory, but potentially from different addresses.
%
Memory locations in shared memory are assigned to~32 ``banks'' in hardware.
%
Each bank can retrieve or store to only one address at a time.
%
All 32 banks can simultaneously deliver words to all 32 threads of a warp quickly, provided that each bank is requested for only one word.
%
When multiple words are requested from the same bank by different threads within a warp, a {\em bank conflict} occurs, causing the bank to access the words sequentially.
%
In the worst case, all 32 threads would read from different addresses within the same bank, and 32 sequential memory accesses would be performed.
%If all threads of a warp access the same word from shared memory, a broadcast operation takes place, where the memory is read once and the value is distributed to all threads. 
%
%This operation is as fast as conflict-free access to every bank.

\paragraph{Global Memory and L1/L2 Cache.}
As with shared memory, when an access to global memory occurs, all active
threads within a warp perform an access to global memory, but potentially on
different addresses.
%
The GPU attempts to coalesce these accesses into actual memory transactions
or {\em sectors},
each of which accesses a fixed number of bytes (e.g., 32 or 128) at a time,
consisting of multiple words.
%
If memory accesses within a warp display a high degree of spacial locality,
the access may need fewer sectors, and therefore a smaller latency, to serve
the requests.
%
In the worst case, if threads within the warp all access very distant
addresses (the memory access is {\em uncoalesced}), many more sectors might
need to be accessed.

The property of global memory described above both compounds and interacts with
the usual effects of caching on memory access latency.
%
GPUs have L1 and L2 caches which behave in a similar way to those of CPUs.
%
As in ordinary CPU programs, memory accesses with a high degree of locality increase the probability of cache hits and therefore better performance.
%
In addition, accesses to the L1 and L2 caches are coalesced in the same way described above, but the size of a sector may differ between the L1 and L2 cache.

%\farzaneh{From NVIDIA:
%Memory accesses that are cached in both L1 and L2 (cached loads using the generic data path) are serviced with 128-byte memory transactions whereas memory accesses that are cached in L2 only (uncached loads
%using the generic data path) are serviced with 32-byte memory transactions. Caching in L2 only can therefore reduce over-fetch, for example, in the case of scattered memory accesses.}




\paragraph{Related work on Modeling Execution Cost of CUDA.}
In prior NSF-supported work~\cite{MullerHo21}, PI Muller and his collaborator Jan Hoffmann developed MiniCUDA, a core calculus for CUDA with a formal dynamic semantics modelling the execution behavior of CUDA kernels on GPUs, including its cost of execution, taking into account factors such as bank conflicts and coalescing of global memory accesses.
%
The work also included a quantitative program logic for predicting the execution cost of a MiniCUDA kernel in terms of a provided {\em resource metric}, which can count any desired combination of bank conflicts, global memory transactions, and other features.
%
We plan to use MiniCUDA as the basis of our language model, and the semantics and quantitative program logic as a starting point for proving the soundness of our type system: our type system should guarantee that two runs of the kernel on states that differ only in high-security memory locations shouldn't differ in their observable execution behavior, including cost.
%
Muller and Hoffmann's implementation of the quantitative program logic, called RaCUDA, can also serve as a starting point for our implementation of the type system.

\paragraph{Related works on GPU security.}
\stefan{TODO Farzaneh: complete}
(With a microbenchmark they measured the
time for a sequence of memory accesses of each thread in a block.)
%
The attack strategy commonly deployed in CPU-based. timing attack methods consist of one block of ciphertext, and profiling the associated time to process that
block. However, on a GPU it would be the encryption workload would contain multiblock messages, and on each data sample, the GPU
timing attackwould produce many blocks of ciphertext.
The key difference is that the GPU scenario will only
collect a single timing value for the multiple blocks.
Although many successful attack methods have been
demonstrated on CPU platforms, these methods cannot be directly applied to the GPU platform due to a
lack of accurate timing information and nondeterminism in thread scheduling.

\paragraph{Symbolic GPU security.}


