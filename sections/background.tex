\label{sec:background}
In this section, we discuss the GPU execution model, focusing on characteristics that lead to vulnerabilities we propose to study.
%
We also discuss prior work on formal models of CUDA and security for GPU programs.
% Extras: 

% One prominent task is inference of neural networks, which process vast amounts of personal data, such as audio, text or images.


% Typically, AI is used for
% the evaluation of large datasets, from processing medical
% data to automated evaluation of surveilance feeds. Today,
% AI suitable for everyday use has arrived at the hands
% of the end-user in the form of Large Language Models
% (LLMs) [

% Hence, a lot of business cases for the utilization of
% LLMs have emerged, ranging from automatically handling
% customer service requests to tasks such as text content
% generation

% offer GPU-asa-
% Service (GaaS), where users can rent GPUs on demand
% and pay via a usage model. Customers can enjoy a high
% degree of flexibility, renting GPU computational power
% on demand.

% Allocating a complete GPU
% to a single container or virtual machine (VM) can be
% considered a waste of resources if either only a fraction
% or only a limited time of processing power is needed. The
% ability to share GPUs gives the provider a cost-effective
% measure to adapt to varying workloads. As industrial
% examples, Google’s Kubernetis Engine allows to share
% a GPU between up to 48 tenants, while Microsoft build
% GPU Paravirtualization into the Hyper-V Hypervisor, allowing
% VMs to share a single GPU



\paragraph{Parallelism and Memory Behavior in GPU.}
Threads on a GPU (of which there may be thousands) are organized into groups
called {\em warps}.
%
We will use the identifier \texttt{warpsize} to refer to the number of threads
per warp, which is~32 on all implementations of which we're aware.
%
Warps are organized into {\em blocks}, and blocks are grouped into {\em grids}. The dimensions of blocks and grids are specified when the kernel is launched.
%
Each thread has a unique thread ID within its block, and each block has a unique block ID within the grid, enabling scalable parallel execution.
%
GPUs utilize Single Instruction, Multiple Thread (SIMT) execution to run the same arithmetic or logical instruction on many threads simultaneously.
%
All threads within a warp must execute the same instruction, although some threads may remain inactive during execution.

Registers are the fastest type of memory in CUDA and are private to each thread, allowing quick access to frequently used variables. 
%
{\em Shared memory} is a smaller, but faster, memory space accessible to all threads within a block, enabling efficient data sharing and coordination. 
%
{\em Global memory} is the largest memory space, accessible by all threads across different blocks, but it is much slower compared to registers and shared memory.

\paragraph{Shared Memory and Bank Conflicts.}
Shared memory, like other types of memory, must be loaded into registers before operations can be performed on it.
%
Because all active threads within a warp perform the same instruction simultaneously, they will all perform simultaneous requests to shared memory, but potentially from different addresses.
%
Memory locations in shared memory are assigned to~32 ``banks'' in hardware.
%
Each bank can retrieve or store to only one address at a time.
%
All 32 banks can simultaneously deliver words to all 32 threads of a warp quickly, provided that each bank is requested for only one word.
%
When multiple words are requested from the same bank by different threads within a warp, a {\em bank conflict} occurs, causing the bank to access the words sequentially.
%
In the worst case, all 32 threads would read from different addresses within the same bank, and 32 sequential memory accesses would be performed.
%If all threads of a warp access the same word from shared memory, a broadcast operation takes place, where the memory is read once and the value is distributed to all threads. 
%
%This operation is as fast as conflict-free access to every bank.

\paragraph{Global Memory and L1/L2 Cache.}
As with shared memory, when an access to global memory occurs, all active
threads within a warp perform an access to global memory, but potentially on
different addresses.
%
The GPU attempts to coalesce these accesses into actual memory transactions
or {\em sectors},
each of which accesses a fixed number of bytes (e.g., 32 or 128) at a time,
consisting of multiple words.
%
If memory accesses within a warp display a high degree of spacial locality,
the access may need fewer sectors, and therefore a smaller latency, to serve
the requests.
%
In the worst case, if threads within the warp all access very distant
addresses (the memory access is {\em uncoalesced}), many more sectors might
need to be accessed.

The property of global memory described above both compounds and interacts with
the usual effects of caching on memory access latency.
%
GPUs have L1 and L2 caches which behave in a similar way to those of CPUs.
%
As in ordinary CPU programs, memory accesses with a high degree of locality increase the probability of cache hits and therefore better performance.
%
In addition, accesses to the L1 and L2 caches are coalesced in the same way described above, but the size of a sector may differ between the L1 and L2 cache.

%\farzaneh{From NVIDIA:
%Memory accesses that are cached in both L1 and L2 (cached loads using the generic data path) are serviced with 128-byte memory transactions whereas memory accesses that are cached in L2 only (uncached loads
%using the generic data path) are serviced with 32-byte memory transactions. Caching in L2 only can therefore reduce over-fetch, for example, in the case of scattered memory accesses.}




\paragraph{Related work on Modeling Execution Cost of CUDA.}
In prior NSF-supported work~\cite{MullerHo21}, PI Muller and his collaborator Jan Hoffmann developed MiniCUDA, a core calculus for CUDA with a formal dynamic semantics modelling the execution behavior of CUDA kernels on GPUs, including its cost of execution, taking into account factors such as bank conflicts and coalescing of global memory accesses.
%
The work also included a quantitative program logic for predicting the execution cost of a MiniCUDA kernel in terms of a provided {\em resource metric}, which can count any desired combination of bank conflicts, global memory transactions, and other features.
%
We plan to use MiniCUDA as the basis of our language model, and the semantics and quantitative program logic as a starting point for proving the soundness of our type system: our type system should guarantee that two runs of the kernel on states that differ only in high-security memory locations shouldn't differ in their observable execution behavior, including cost.
%
Muller and Hoffmann's implementation of the quantitative program logic, called RaCUDA, can also serve as a starting point for our implementation of the type system.

\farzaneh{Are there any particular citations that we can add for the above discussion?}

\paragraph{Related works on GPU security.}
\stefan{TODO Farzaneh: complete}

\paragraph{Symbolic GPU security.}
~\cite{horga2022JSA}

Static detection of uncoalesced accesses in GPU programs
GKLEE: concolic verification and test generation for GPUs
Detecting bank conflict of GPU programs using symbolic execution—Case study
The design and implementation of a verification technique for GPU kernels
SMT-based context-bounded model checking for CUDA programs
Specification and verification of GPGPU programs


\paragraph{GPU performance.}
Genetic algorithm based estimation of non–functional properties for GPGPU programs: has targeted such a side channel through the use of genetic-algorithm-powered test input generation.
S. Hong, H. Kim, An analytical model for a GPU architecture with memory-level and thread-level parallelism awareness, in: Proceedings of the 36th Annual
S.S. Baghsorkhi, M. Delahaye, S.J. Patel, W.D. Gropp, W.-m.W. Hwu, An adaptive performance modeling tool for GPU architectures, in: Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, 2010, pp. 105-114.
Predicting GPU performance from CPU runs using machine learning
A comparison of GPU execution time prediction using machine learning and analytical modeling
A performance model for GPUs with caches



% \subsection{Prior work}
% Prior work has explored various approaches to prevent these vulnerabilities, taking into account both the private information stored in shared memory and that in global memory.
% %
\paragraph{Leakage via shared and global memory.} Pietro et al.~\cite{pietro2016TECS} proposed to rewrite the code such that it zeroizes all leftover data in shared memory after kernel execution terminates.
%
They demonstrated this approach with a sample program that calculates the sum of two vectors.
%
Additionally, they implemented a zeroizing function within the CUDA runtime to clear all global memory after the context ends.
%
In their experiments, they found that the overhead of zeroizing shared memory was negligible for the sample program.
%
However, the overhead of zeroizing global memory through the CUDA runtime was not negligible.
%
Even if the overhead for zeroizing might be low, their approach prevents low-security data from remaining in shared memory when necessary; it forces the user to transfer the low-security data between CPU and GPU for each kernel execution, disrupting performance optimizations related to data locality and defeating the purpose of GPU programming.
%

%
% Furthermore, their approach toward zeroizing shared memory is somewhat ad-hoc, relying completely on the programmer to rewrite the program to zeroize the shared memory location.
% %

% The overhead of zeroizing might be low (how much?) but still it doesn't allow low security ones to remain in the shared memory, when one needed.
%

\paragraph{Dynamic taint tracking for leakage via global memory.} Hayes et al.~\cite{hayes2017usenix}, introduced a dynamic taint tracking approach as a complementary measure to identify memory locations tainted with sensitive data.
%
They then use that taint information either to trigger an alarm whenever there is an aunauthorized access to such tainted memory or to zeroize the tainted locations in shared (resp. global) memory after the kernel (resp. context) terminates.
%
Dynamic taint tracking is known to be an expensive procedure. 
%
Hayes et al. optimized it by leveraging GPU-specific features, such as eliminating certain instructions working with runtime parameters and constants from taint tracking, as well as utilizing the large GPU register file.
%
These optimizations significantly reduced the overhead compared to CPU-based taint-tracking algorithms,  achieving a 5-20x speedup across a range of applications.
%
Despite these improvements, the approach still incurs a 5\%-13\% performance penalty, particularly when dealing with shared memory.

% However, none of the prior work address the integrity aspect of the attack, where an attacker could leave malicious data behind in shared memory, potentially compromising the system’s security.

% None of the prior work considers that low-integrity writes can be as malicious to security as low-security reads.
% %
% This is especially concerning when dealing with an active attacker on the GPU, who can injects data into memory to force leaking sensitive information,  e.g., an attacker can rewrite the cipherkey behind in shared memory and observe the resulting ciphertext.


\paragraph{Relational Hoare Logic.}
\farzaneh{Put more here based on Amal and Jan's RHL.}


\paragraph{Quantitaive security analysis}

\paragraph{Static and dynamic taint tracking}

\paragraph{CPU timing channels}
Automatic quantification of cache side-channels, Cacheaudit: A tool for the static analysis of cache side channels, Multi-run side-channel analysis using symbolic execution and max-SMT, Quantifying the information leakage in cache attacks via symbolic execution



\farzaneh{I'm not sure what the following refers to. drop?}
(With a microbenchmark they measured the
time for a sequence of memory accesses of each thread in a block.)
%
The attack strategy commonly deployed in CPU-based. timing attack methods consist of one block of ciphertext, and profiling the associated time to process that
block. However, on a GPU it would be the encryption workload would contain multiblock messages, and on each data sample, the GPU
timing attackwould produce many blocks of ciphertext.
The key difference is that the GPU scenario will only
collect a single timing value for the multiple blocks.
Although many successful attack methods have been
demonstrated on CPU platforms, these methods cannot be directly applied to the GPU platform due to a
lack of accurate timing information and nondeterminism in thread scheduling.
