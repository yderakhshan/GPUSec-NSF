\subsection{Motivating Example}
% \farzaneh{move to the intro if needed: GPU memory is split into several regions, onchip and off-chip. On-chip memory consists of registers,
% caches, and shared memory.
% Off-chip memory
% is GDDR SGRAM, which is logically distributed into
% texture memory, constant memory, local memory, and
% global memory.
% Both texture memory and constant memory are read-only during the GPU kernel execution. Local memory is private to each thread and thus does not play a role in any of the two attack models. Therefore, in this proposal we focus on registers, shared memory, and global memory.
% }
% \farzaneh{Merge the paragraph here or add to the first thrust:
% The victim sends its plaintext to the server through a secure channel that the attacker cannot observe.
% % 
% The server, then, uses a private cipher key to encrypt the plaintext into a ciphertext that is public and, therefore, visible to the attacker.
% %
% The attacker, using the same server for a different purpose, aims to get access to either the victim's plaintext or the cipher key used by the server;  accessing either of these would allow the attacker to decrypt the plaintext, as it can observe the public ciphertext.
% % 
% Even accessing a few words of plaintext or the cipher key is regarded as a security leak, as it allows the attacker to execute a faster brute-force attack on the remaining data.
% }
% \farzaneh{Amdahl's law + GPUs are expensive -> one way is to share, e.g., MPS}

In this thrust, we consider the scenario in which both the victim and attacker are clients of a shared GPU (see Figure~\ref{fig:th2-attack}). 
%
Several prior sudies~\cite{lee} showed that if the attacker's application runs on the same GPU as the victim's, it can potentially access private information left on the GPU memory by the victim's application.
%
This potential leak occurs both in a sequential time-sharing setting and in a concurrent setting, e.g., enabled by NVIDIA Multi-Process Service (MPS).
%
In a sequential setting, multiple GPU processes are executed in an interleaved fashion, with each process having exclusive access to GPU resources for a defined duration. The kernels of these processes are scheduled to run one after the other.
%
In the concurrent setting, multiple GPU processes can share GPU resources simultaneously, enabling multiple kernels to run at the same time.
%
In both cases, a lack of proper memory isolation may lead to an overlap between the memory locations that the attacker can access through normal operations—without any special privileges—and those used by the victim.
%
%
% If the attacker's application runs concurrently with the victim's, it can potentially access private information that is left on the GPU memory by the victim's application.
%
% if the memory locations allocated to the attacker overlap with those used by the victim.
%
% To achieve this, the attacker only needs to access the GPU memory through regular allocation, deallocation, and read/write operations without requiring any special privileges.
%
The details of the attack vary depending on where in the memory the victim's information is located, e.g., in the shared or global memory, and whether the GPU server is concurrent or sequential.
%

\paragraph{Time-sharing setting.}

Figure~\ref{fig:th2-attack}(a), for example, demonstrates an attack on a sequential server with private information stored in shared memory.
%
Similar to Thrust 1, the victim in this scenario is an AES application.
% 
GPU computing models discourage long-running GPU kernels. 
%
Thereby, the AES application uses several kernels, instead of a long-running one, repeatedly and processes the intermediate results. 
%
To increase efficieny, the application keeps its frequently accessed data, e.g., the encryption key, in shared memory, even after a single kernel terminates.
%
This means that if an attacker's kernel is scheduled to run between two kernels of the victim, it can compromise the security of the plaintext. 
%
The attacker can compromise the security by either reading the data left in shared memory, eventually relaying it back to the attacker's CPU (through either a direct or indirect leak), or by overwriting the data, altering the key.
%
In both cases, the attacker knows the key and can infer the plaintext by observing the public cyphertext. 
%
In the literature, this attack is often called the End of Kernel (EOK) attack.


Figure~\ref{fig:th2-attack}(b) demonstrates a similar attack for a sequential server, but this time with the private information stored in global memory.
%
A segment of global memory is allocated to each CUDA context,  and an interleaved attacker kernel cannot access it until the application has terminated and the memory is deallocated.
%
After the application terminates, it deallocates the memory,  which then becomes available for future allocations.
%
However, prior work~\cite{pietro2016TECS} have demonstrated that data can persist even after deallocation, allowing an attacker kernel that runs immediately after the application to access the memory segment with its prior data. 
%
As such, even if the programmer attempts to clear the memory before exiting, this access can still result in information leakage. 
%
For example, in Figure~\ref{fig:th2-attack}(b), the attacker can access the plaintext stored in the global memory by the victim even after the deallocation of the global memory segment by the victim.

\paragraph{Spatial-sharing setting.}

When it comes to concurrent access, the situation becomes more complex as resources are allocated dynamically, and two applications can, in practice, access one segment of the memory simultaneously (see Figure~\ref{fig:th2-attack}(c)).
%
As stated in NVIDIA's MPS manual (Section 2.3.3.1), ``An out-of-range read in a CUDA kernel can access CUDA-accessible memory modified by another process and will not trigger an error, leading to undefined behavior.'' \cite{hayes2017usenix}
\farzaneh{I'm not aware of many prior work on such attacks in MPS}



% Processes sharing the GPU send their requests to the MPS, where these requests are queued and processed by the GPU using a FIFO scheduling policy.
% 
% MPS is designed to enhance performance when a single application process underutilizes the GPU's compute capacity and thus the GPU is shared between multiple processes.
% 
% The operating system schedules the task based on the scheduling policy. 
% GPU computing models discourage long-running GPU kernels because current GPUs do not support preemptive scheduling. Long-running GPU programs thereby use either several kernels or the same kernel repeatedly and process the intermediate results. The main target of this attack are frequently accessed data stored in the per-CU local and per-PE private memory. For example it can load secret keys, AES S-Boc and ... in the local and private memories at the end of each kernel execution. An attacker can easily read this data.
% %
% The memory allocated by any of the client processes is accessible to all other client processes, which compromises memory isolation between them.
%

\subsection{Prior work}
Prior work has explored various approaches to prevent these vulnerabilities, taking into account both the private information stored in shared memory and that in global memory.
%
Pietro et al.~\cite{pietro2016TECS} proposed to rewrite the code such that it zeroizes all leftover data in shared memory after kernel execution terminates.
%
They demonstrated this approach with a sample program that calculates the sum of two vectors.
%
Additionally, they implemented a zeroizing function within the CUDA runtime to clear all global memory after the context ends.
%
In their experiments, they found that the overhead of zeroizing shared memory was negligible for the sample program.
%
However, the overhead of zeroizing global memory through the CUDA runtime was not negligible.
%
Even if the overhead for zeroizing might be low, their approach prevents low-security data from remaining in shared memory when necessary; it forces the user to transfer the low-security data between CPU and GPU for each kernel execution, disrupting performance optimizations related to data locality and defeating the purpose of GPU programming.
%
% Furthermore, their approach toward zeroizing shared memory is somewhat ad-hoc, relying completely on the programmer to rewrite the program to zeroize the shared memory location.
% %

% The overhead of zeroizing might be low (how much?) but still it doesn't allow low security ones to remain in the shared memory, when one needed.
%
Hayes et al.~\cite{hayes2017usenix}, introduced a dynamic taint tracking approach as a complementary measure to identify memory locations tainted with sensitive data.
%
They then use that taint information either to trigger an alarm whenever there is an aunauthorized access to such tainted memory or to zeroize the tainted locations in shared (resp. global) memory after the kernel (resp. context) terminates.
%
Dynamic taint tracking is known to be an expensive procedure. 
%
Hayes et al. optimized it by leveraging GPU-specific features, such as eliminating certain instructions working with runtime parameters and constants from taint tracking, as well as utilizing the large GPU register file.
%
These optimizations significantly reduced the overhead compared to CPU-based taint-tracking algorithms,  achieving a 5-20x speedup across a range of applications.
%
Despite these improvements, the approach still incurs a 5\%-13\% performance penalty, particularly when dealing with shared memory.

% However, none of the prior work address the integrity aspect of the attack, where an attacker could leave malicious data behind in shared memory, potentially compromising the system’s security.

None of the prior work considers that low-integrity writes can be as malicious to security as low-security reads.
%
This is especially concerning when dealing with an active attacker on the GPU, who can injects data into memory to force leaking sensitive information,  e.g., an attacker can rewrite the cipherkey behind in shared memory and observe the resulting ciphertext.


\subsection{Proposed work}
We propose two approaches to prevent this kind of attack.
%
In Task 2.a, we propose an approach based on static information flow control analysis.
%
We design a flow-sensitive taint tracking system that captures the flow of sensitive data throughout the program and memory locations statically via taint levels.
%
Compared to Thrust 1, in this thrust, we need to consider the interleaving between multiple warps within a block (which share the shared memory) and multiple blocks within a kernel (which share global memory).
% %
% This imposes a challenge, as the warps withing a block and blocks within a kernel can be interleaved by a scheduler.
%
The interleaving of warps and blocks is controlled by a scheduler, the exact details of which are not fully known due to the proprietary nature of NVIDIA's implementation and the dependence on runtime events.
% 
As a result, we must assume that any interleaving of warps and blocks is valid.
% 
We model the system with a non-deterministic scheduler and aim to prove that, for any possible interleaving of warps and blocks, the program is secure.

While this static approach is low-cost, it only provides an over-approximation of the security guarantees. 
% 
For example, when two warps both access the same shared memory location—one for reading and the other for writing—the static analysis must assume that if the write is high-security, the read is considered tainted. 
% 
However, in practice, if the read always happens before the write, there may be no need to taint the data, as no leak would occur in such a case.

In Task 2.b, we address the limitations of the static analysis approach by incorporating a hybrid method that combines both static and dynamic taint tracking.
% 
% Dynamic taint tracking is inherently expensive, especially if we track more than just a single bit for each memory location.
% Rather than a binary secure/insecure flag, we aim to capture the full spectrum of information flow within the application.
%
To achieve this, we propose using static analysis for each warp (with respect to shared memory) and each block (with respect to global memory) to establish the sensitive flow of information in these units.
%
The we design a dynamic monitoring to track the actual interleaving of warps and blocks at runtime.
% 
The dynamic taint tracking hardware monitor should detect potential insecure memory accesses, either preventing them or raising an alarm.
% 
Additionally, this monitor can sanitize sensitive (tainted) data objects at the end of their lifecycle.

%
% We plan to use static analysis for each warp and each block and then capture the interleaving using the dynamic monitor.

In Task 2.c, we plan to implement a Taint Analysis Framework based on the ideas of Tasks 2.a and 2.c using OCAML.
\farzaneh{I'm not sure about this.}

In the proposal we describe our ideas based on the EoK attacks in the time-sharing setting. 
% In thrust 1, we propose an approach based on a static information flow control analysis in which we control the flow.
% %
% It is similar to the previous task, except that we need to consider multiple warps in a block that share the shared memory and multiple blocks in the kernel that share the global memory. 
% %
% The warp schedulers draw from a pool of available/ready warps, and select one or more instruction, per cycle, from each warp, per schedule
% %
% Any possible interleaving of blocks should be valid. presumed to run to completion without pre-emption can run in any order. can run concurrently OR sequentially
% Threads are assigned to Streaming
% Multiprocessors (SM) in block granularity
% Each Block is executed as 32-thread Warps
% , we need to put severe restrictions on the programs, restricting them to store any sensitive information in either shared or global memory. 
% % 
% We, instead, propose a dynamic taint tracking hardware monitor to catch such potential insecure memory accesses, either prevent them or raise an alarm, and changes the route of scheduling, and clears sensitive (tainted) data objects at the end of their life.
%
% The taintable sources are program inputs
% %
% In Thrust 2, we propse a dynamic taint tracking system, considering the expensive.
% %
% Dynamic taint tracking is expensive, particularly if we don't want to consider a single bit and do genreic taint tracking, i.e., instead of only one bit of secure/insecure, capture the full lattice of applications.
% % 
% In prior work they considered making it better in performance by considering things specific to GPU, for example specifying that tid or constant is never tainted.
% %
% We propose a new approach based on combining static analysis and dynamic taint tracking.
% %
% For the static analysis we can capture the flow of information inside the victim's application kernel.
% %
% With this we know exactly which parts are tainted and which parts are not.


% Programmers can manually erase global memory before program exit, but registers and local memory are allocated by the compiler and cannot be as easily cleared.

% optimize the data locality
% minimize data trasnfer between CPU and GPU and between peer GPUs.
% use shared memory for data frequenlty used within SM.
% This severly affects performance and defeats the purpose of GPU programming. 
% %


% optimize the data locality
% minimize data trasnfer between CPU and GPU and between peer GPUs.
% use shared memory for data frequenlty used within SM.

% the warp schedulers draw from a pool of available/ready warps, and select one or more instruction, per cycle, from each warp, per schedule

% Any possible interleaving of blocks should be valid. presumed to run to completion without pre-emption can run in any order. can run concurrently OR sequentially
% Threads are assigned to Streaming
% Multiprocessors (SM) in block granularity
% Each Block is executed as 32-thread Warps

% within a block, threads share data via
% shared memory
% Data is not visible to threads in other blocks




% We propose two approaches to preven this kind of attack.
% %
% The first approach is a static one based on a static type analysis , we need to put severe restrictions on the programs, restricting them to store any sensitive information in either shared or global memory. 
% % 
% This severly affects performance and defeats the purpose of GPU programming. 
% %
% Particularly, when we cannot rely on the attacker's programs to be statically typed.
% %
% We, instead, propose a dynamic taint tracking hardware monitor to catch such potential insecure memory accesses, either prevent them or raise an alarm, and changes the route of scheduling, and clears sensitive (tainted) data objects at the end of their life.
% %
% % The taintable sources are program inputs
% % %

% Dynamic taint tracking is expensive, particularly if we don't want to consider a single bit and do genreic taint tracking, i.e., instead of only one bit of secure/insecure, capture the full lattice of applications.
% % 
% In prior work they considered making it better in performance by considering things specific to GPU, for example specifying that tid or constant is never tainted.
% %
% We propose a new approach based on combining static analysis and dynamic taint tracking.
% %
% For the static analysis we can capture the flow of information inside the victim's application kernel.
% %
% With this we know exactly which parts are tainted and which parts are not.
% Since the applications are scheduled nondeterministically though, and can be used by different clients, we cannot use static taint tracking for interleaving applications in an efficient way. Otherwsise it will be too restrictive.
% %
% We built upon the static information flow control built prior.
% This happens when both kernels are accessing the shared memory, and one is safe while the other one is not. written in shared, is already given by the first thrust.

% It enables data protection that clears sensitive (tainted) data objects at the end of their life range as well as detects leak of the sensitive data in the midst of program execution.


% Examples include face recognition photos, a plain-text message, and encryption key.
% It enables data protection that clears sensitive (tainted) data objects at the end of their life range as well as detects leak of the sensitive data in the midst of program execution.


% %
% The Multi-Process Service (MPS) is a software solution designed to efficiently manage multiple processes sharing a GPU allowing to run them concurrently~\cite{anasic2014CAN, NVDIA2013, li2011ICPP}.
% %
% % It is designed to enhance performance when a single application process underutilizes the GPU's compute capacity.
% %
% Processes sharing the GPU send their requests to the MPS, where these requests are queued and processed by the GPU using a FIFO scheduling policy.
% % 
% MPS is designed to enhance performance when a single application process underutilizes the GPU's compute capacity and thus the GPU is shared between multiple processes.
% %
% However, one of its key limitations is that memory allocated by any of the client processes is accessible to all other client processes, which compromises memory isolation between them.
% %
% % New requests can be accepted even while another application is currently executing a kernel on the GPU.


\input{sections/thrust2/2a.tex}
\input{sections/thrust2/2b.tex}
\input{sections/thrust2/2c.tex}
